{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L09 - DNN\n",
    "\n",
    "## Author - Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem:\n",
    "Using the CIFAR-10 dataset, create a new notebook to build a TensorlLow model.\n",
    "\n",
    "\n",
    "# Abstract:\n",
    "You start working for a new startup building the next generation search engine. The search engine provides the ability to search images with their content. You are tasked to build a machine learning model that is able to identify the objects in images. The model you are building will help in providing the capability to search for 10 objects. Download the L09_ImageClasses.pdf to see a list of the classes in the dataset and 10 random images from each class.\n",
    "\n",
    "For this project you will use the CIFAR-10 dataset, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The analysis is is divided the following way:\n",
    "\n",
    "### Data Exploration\n",
    "- **Visual Exploration of the variables**\n",
    "    - Categorical Variable\n",
    "\n",
    "### Analysis\n",
    "- **SVC review**\n",
    "    - Split Data Set\n",
    "        \n",
    "### Summary of Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read CIFAR-10 dataset from Keras.\n",
    "- Explore data\n",
    "- Preprocess and prepare data for classification\n",
    "- Build a TensorFlow model using a single dense hidden layer\n",
    "- Apply model to test set and evaluate accuracy\n",
    "- Perform 3 adjusts to the number of layers and activation functions to improve accuracy\n",
    "- Summarize your findings regarding the different iterations and any insights gained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes in the data set represent the following:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the image is 255 x 255 pixels. As a result, we will scale the values to range between 0 and 1, and thus we will divide by 255.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting names of target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation for the used data set from Keras more descriptive names were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_function(y_train):\n",
    "    names = []\n",
    "    for i in y_train:\n",
    "        if i == 1:\n",
    "            j = 'car'\n",
    "        elif i == 2:\n",
    "            j = 'bird'\n",
    "        elif i == 3:\n",
    "            j = 'cat'\n",
    "        elif i == 4:\n",
    "            j = 'deer'\n",
    "        elif i == 5:\n",
    "            j = 'dog'\n",
    "        elif i == 6:\n",
    "            j = 'frog'\n",
    "        elif i == 7:\n",
    "            j = 'horse'\n",
    "        elif i == 8:\n",
    "            j = 'ship'\n",
    "        elif i == 9:\n",
    "            j = 'truck'\n",
    "        elif i == 0:\n",
    "            j = 'airplane'\n",
    "        names.append(j)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_train = names_function(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_test = names_function(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(names_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment/example even though a better approach would have been to do a **grid search** to find the optimal hyperparameters, just to show and exemplify how the model will vary based on these values and the importance of it 3 slightly different cases/models + 1 baseline (1 Hidden layer neural network) would be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **TensorFlow** library these are some of the options available:\n",
    "\n",
    "### Optimizer options:\n",
    "\n",
    "- `sgd`: Gradient Descent with momentum\n",
    "- `rmsprop`: Optimizer that implements the RMSprop algorithm\n",
    "- `optimizer`: Base class for Keras\n",
    "- `nadam`: NAdam algorithm\n",
    "- `ftrl`: FTRL algorithm\n",
    "- `adam`: Adam algorithm\n",
    "- `adagrad`: Adagrad algorithm\n",
    "- `adadelta`: Adadelta algorithm\n",
    "\n",
    "\n",
    "### Loss options (for classification):\n",
    "\n",
    "- `BinaryCrossentropy` class\n",
    "- `CategoricalCrossentropy` class\n",
    "- `SparseCategoricalCrossentropy` class\n",
    "- `Poisson` class\n",
    "- `binary_crossentropy` function\n",
    "- `categorical_crossentropy` function\n",
    "- `sparse_categorical_crossentropy` function\n",
    "- `poisson` function\n",
    "- `KLDivergence` class\n",
    "- `kl_divergence` function\n",
    "\n",
    "### Metrics (accuracy):\n",
    "\n",
    "- `Accuracy` class\n",
    "- `BinaryAccuracy` class\n",
    "- `CategoricalAccuracy` class\n",
    "- `TopKCategoricalAccuracy` class\n",
    "- `SparseTopKCategoricalAccuracy` class\n",
    "\n",
    "**Others:**\n",
    "\n",
    "- `AUC` class\n",
    "- `Precision` class\n",
    "- `Recall` class\n",
    "- `TruePositives` class\n",
    "- `TrueNegatives` class\n",
    "- `FalsePositives` class\n",
    "- `FalseNegatives` class\n",
    "- `PrecisionAtRecall` class\n",
    "- `SensitivityAtSpecificity` class\n",
    "- `SpecificityAtSensitivity` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(100, activation='tanh'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer= 'sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "x = model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Model\n",
    "\n",
    "For this model 3 small changes were done:\n",
    "- The number of layers\n",
    "- The number of neurons\n",
    "- The activation function from `tanh` to `relu`\n",
    "- The optimizer from `sgb` to `nadam`\n",
    "\n",
    "**Note:** \n",
    "As mentioned above ideally a more robust approach should be taken such as grid search or random search to find the best hyperparameters as well as the number of layers and neurons per layer and the type of layer, but since the purpose this assignment is to exemplify the use of tensorflow and keras a more simplistic approach would be followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model3 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.fit(x_train, y_train, epochs=2)\n",
    "\n",
    "model1 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(1000, activation='tanh'),\n",
    "    keras.layers.Dense(500, activation='tanh'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer= 'sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model1.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='nadam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model3 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(np.argmax(predictions[i]), 100*np.max(predictions), names_test[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
