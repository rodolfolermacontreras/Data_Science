{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Use this notebook to follow along with the lab tutorial.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 9 - Deep Learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN for Adding Binary Bits\n",
    "This notebook adds up the 8 bits of a byte into a final number. It shows the simplicity of taking in sequencial data and converting that into information to generate an output. \n",
    "\n",
    "NOTE: bits are built up in reverse order so we start with the end and then add the next two numbers and the next two numbers and so on until we reach the final solution. \n",
    "\n",
    "A special shout out to <a href=\"https://iamtrask.github.io/\">**I Am Trask**</a> who wrote the original version of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Activation Function and the Backpropgation\n",
    "We are creating the \"brain\" of the RNN--the s-haped activation function (TanH) and the backpropagation through time (BPTT) for updating the weights which is the derivative values of the output from the \"squashed\" activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tanH (s-shaped) activation function \n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# compute the BPTT\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 8-Bits Training Dataset\n",
    "This section generates a dictionary of 8 bit bytes for our training and testing use. It also sets the binary dimension which will becoem relevant in the training phase to cycle through individual bits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Number of numbers in our numpy array = 256\n",
      "Largest value = 256\n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8 # NOTE this will be used later for the training phase\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "print(type(binary))\n",
    "print(\"Number of numbers in our numpy array =\", len(binary))\n",
    "print(\"Largest value =\", largest_number)\n",
    "print(binary[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the RNNs hyperparameters\n",
    "This section is where we can set (and test other configurations of) the RNNs hyperparameters\n",
    "\n",
    "* alpha -- this is where we set the learning rate to various numbers and test, from here, what is the best one for the problem. \n",
    "* input_dim -- width of the input vector. This is the length of the sentence, phrase, word, etc. that you are putting into the system. In this example we are adding two numbers so we need just 2. \n",
    "* hidden_dim -- the vector to store the weights internally. This, like alpha, is a hyperparameter to modify/test for improved accuracy--in general this is larger than the input vector and not arbitrarily large given the affect on computational performance.\n",
    "* output_dim -- the size of the output vector which in this case is 1\n",
    "* epochs -- the number of times we loop through all of the training examples to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1       # learning rate\n",
    "input_dim = 2     # input dimension -- the length of the \"sentence\"\n",
    "hidden_dim = 16   # width of the hidden layer -- 2x the size of our max length of inputs\n",
    "output_dim = 1    # size of what we want to return\n",
    "epochs = 10000    # number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the RNN Synapses\n",
    "This is where we initialize the weights between the input, hidden and output layers--the memory of the network to random numbers. We multiply the random number (a float between zero and 1) and substract one to ensure they range between -1 to 1 like our tanH activation function. \n",
    "* synapse_0 -- weights between the input and hidden layer; size is input by hidden\n",
    "* synapse_1 -- weights between the hidden and output layer; size is hidden by output\n",
    "* synapse_2 -- weights between the hidden layer and the previous time step -- the \"loop back\"; size is hidden by hidden\n",
    "\n",
    "Bacause we are remembering through time, we need to store the updates that accumulate for these these weights. We are initializing these to zero because they will get assigned the synapse weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09762701  0.43037873  0.20552675  0.08976637 -0.1526904   0.29178823\n",
      "  -0.12482558  0.783546    0.92732552 -0.23311696  0.58345008  0.05778984\n",
      "   0.13608912  0.85119328 -0.85792788 -0.8257414 ]\n",
      " [-0.95956321  0.66523969  0.5563135   0.7400243   0.95723668  0.59831713\n",
      "  -0.07704128  0.56105835 -0.76345115  0.27984204 -0.71329343  0.88933783\n",
      "   0.04369664 -0.17067612 -0.47088878  0.54846738]]\n"
     ]
    }
   ],
   "source": [
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "print(synapse_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go through the training the RNN, let's understand the problem a bit better. \n",
    "\n",
    "The first thing we need is a \"new\" number. We also need the binary representation of that number. So we generate a random number and divide it by 2. The reason we halve it is because we are adding to binary numbers which if we don't start from half or less, could exceed the 8-bits.\n",
    "\n",
    "The second thing we do is lookup the binary value using int2binary. Now we have our two numbers to add together. \n",
    "\n",
    "Below I'm showing an example \"num\" which will become variables \"a\" and \"b\" in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[0 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# generate a simple addition problem (a + b = c)\n",
    "num_int = np.random.randint(largest_number/2) # int version\n",
    "print(num_int)\n",
    "num = int2binary[num_int] # binary encoding\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"right answer\" is the addition of two values. In the loop this is \"c\". In the example I'm using \"sum\" and I'll simply add \"num\" to itself. \"c\" is the *Y* or actual value. We also need \"d\" the *Y-hat*, or predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[0 0 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "sum_int = num_int + num_int\n",
    "print(sum_int)\n",
    "sum = int2binary[sum_int]\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "This is where we generate the problem and train the RNN on how to solve it. \n",
    "\n",
    "NOTE: instead setting the problem values in the outer loop, \"j\", we could have set up an array of values and used the inner loops to cycle through the array's dimensions, but including it in the outer loop is more parsimonious. As a result we have to reinitalize the variables for storage for each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[4.17781292]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 0 1 1 1]\n",
      "60 + 91 = 0\n",
      "\n",
      "Error:[4.00100162]\n",
      "Pred:[0 1 1 1 1 0 1 1]\n",
      "True:[0 1 0 1 1 0 0 0]\n",
      "35 + 53 = 123\n",
      "\n",
      "Error:[4.08606799]\n",
      "Pred:[1 1 1 1 1 1 1 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "44 + 90 = 254\n",
      "\n",
      "Error:[3.76658514]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[1 0 1 1 0 1 1 0]\n",
      "73 + 109 = 255\n",
      "\n",
      "Error:[3.97578327]\n",
      "Pred:[1 1 1 1 0 1 0 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "11 + 123 = 244\n",
      "\n",
      "Error:[2.11727527]\n",
      "Pred:[0 1 1 1 0 1 1 0]\n",
      "True:[0 1 1 1 0 1 1 0]\n",
      "113 + 5 = 118\n",
      "\n",
      "Error:[1.44864506]\n",
      "Pred:[0 0 0 1 1 1 1 0]\n",
      "True:[0 0 0 1 1 1 1 0]\n",
      "0 + 30 = 30\n",
      "\n",
      "Error:[1.09735237]\n",
      "Pred:[1 1 0 1 0 1 0 0]\n",
      "True:[1 1 0 1 0 1 0 0]\n",
      "125 + 87 = 212\n",
      "\n",
      "Error:[0.47673483]\n",
      "Pred:[0 1 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 0 0 1]\n",
      "17 + 48 = 65\n",
      "\n",
      "Error:[0.6482567]\n",
      "Pred:[0 1 1 1 1 0 0 1]\n",
      "True:[0 1 1 1 1 0 0 1]\n",
      "3 + 118 = 121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(epochs):\n",
    "    \n",
    "    '''Initialization'''\n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer - the \"Y\"\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int] \n",
    "    \n",
    "    # where we'll store our best guess (binary encoded) - the \"Y-hat\" of predicted values\n",
    "    d = np.zeros_like(c) # initialize to zero Y-hat array of predicted values\n",
    "\n",
    "    overallError = 0 # initalize error value for each epoch to monitor the convergence\n",
    "    \n",
    "    # initialize lists used to keep track of the layer 2 derivatives and layer 1 values at each time step\n",
    "    layer_2_deltas = list()                     # derivatives from priors - layer 2\n",
    "    layer_1_values = list()                     # values from layer 1\n",
    "    layer_1_values.append(np.zeros(hidden_dim)) # append zeros to store\n",
    "    '''End Initialzation'''\n",
    "    \n",
    "    # moving along the positions in the binary encoding -- right to left\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        # X list of a and b (in binary), indexed with the farthest right as zero\n",
    "        # y is the correct answer (in binary), indexed the same \n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T # transpose the array\n",
    "\n",
    "        '''Construct hidden layer'''\n",
    "        # propagate input to the hidden layer (X,synapse_0)\n",
    "        # propagate *previous* hidden layer to the current hidden layer(prev_layer_1, synapse_h)\n",
    "        # sum these two vectors\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        '''Construct output layer'''\n",
    "        # propagate hidden layer to the output --> make a prediction\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        '''Verify results'''\n",
    "        # determine how far predicted is from actual, store the derivative and calculate error\n",
    "        layer_2_error = y - layer_2 # comparison\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2)) # store derivative at this timestep\n",
    "        # save this to show it at the end\n",
    "        overallError += np.abs(layer_2_error[0]) # calculate sum of errors\n",
    "    \n",
    "        '''For logging progress'''\n",
    "        # decode estimate so we can print it out at the end\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        '''Set up for next pass'''\n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "        \n",
    "    # initialize future_layer -- reset \n",
    "    future_layer_1_delta = np.zeros(hidden_dim) \n",
    "    \n",
    "    '''Generate FF Loop'''\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input (X) and output (y)\n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        \n",
    "        '''Access current (time) hidden layer'''\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        \n",
    "        '''Access previous hidden layer'''\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        '''Get the output error'''\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        \n",
    "        '''Generate derivative for BPTT'''\n",
    "        # compute error at current hidden layer given future layer and current output layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        '''BPTT--update the weights'''\n",
    "        # Update weights between input hidden and output layers\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        '''Store the future layer difference'''\n",
    "        # Update the error rate in the future prediction as an input to the backpropgation step for next time\n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "    # Update the Weights\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    # Reset the update variables to zero\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Please see the Wine Neural Net notebook for your opportunity to try for yourself.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
