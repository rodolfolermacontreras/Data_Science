{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10 - RNN\n",
    "\n",
    "## Author - Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem:\n",
    "Using the Keras dataset, create a new notebook and perform each of the following data preparation tasks and answer the related questions:\n",
    "\n",
    "- Read Reuters dataset into training and testing \n",
    "- Prepare dataset\n",
    "- Build and compile 3 different models using Keras LTSM ideally improving model at each iteration.\n",
    "- Describe and explain your findings.\n",
    "\n",
    "# Abstract:\n",
    "Your next generation search engine startup was successful in having the ability to search for images based on their content. As a result, the startup received its second round of funding to be able to search news articles based on their topic. As the lead data scientist, you are tasked to build a model that classifies the topic of each article or newswire. \n",
    "\n",
    "For this assignment, you will leverage the RNN_KERAS.ipynb lab in the lesson. You are tasked to use the Keras Reuters newswire topics classification dataset. This dataset contains 11,228 newswires from Reuters, labeled with over 46 topics. Each wire is encoded as a sequence of word indexes. For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "The analysis is is divided the following way:\n",
    "\n",
    "### Data Exploration\n",
    "- **Looking at an example**\n",
    "\n",
    "\n",
    "### Analysis\n",
    "- **Processing the data**\n",
    "- **Training variables (*hyperparameters*)**\n",
    "- **RNN Model**\n",
    "    - Based Model\n",
    "    - 2nd Model\n",
    "    - 3rd Model\n",
    "- **Results**\n",
    "\n",
    "        \n",
    "### Summary of Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "#import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ly266e\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\ly266e\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "num_of_words = 10000\n",
    "test_split_size = 0.2\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data(\n",
    "    path=\"reuters.npz\",\n",
    "    num_words=num_of_words,\n",
    "    test_split=test_split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we have numbers and not words, as the words had been indexed by overall frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training Samples: 8982\n",
      "# of Test Samples: 2246\n",
      "# of Classes: 46\n"
     ]
    }
   ],
   "source": [
    "print('# of Training Samples: {}'.format(len(x_train)))\n",
    "print('# of Test Samples: {}'.format(len(x_test)))\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "print('# of Classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.reuters.get_word_index()\n",
    "# word_index = tf.keras.datasets.reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index[\"<UNK>\"] = 0\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Review \n",
      "\n",
      "the of of mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n",
      "\n",
      "Its label : 3\n"
     ]
    }
   ],
   "source": [
    "print('\\nFirst Review \\n')\n",
    "print(decode_review(x_train[0]))\n",
    "print('\\nIts label :',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 256\n",
    "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test_padded  = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "classes = 46\n",
    "y_train_sparse = keras.utils.to_categorical(y_train, num_classes=classes)\n",
    "y_test_sparse = keras.utils.to_categorical(y_test, num_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training variables (*hyperparameters*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training varibles\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.0001\n",
    "batch_size = 512\n",
    "epochs = 50 #Even though this would be computationally expensive, since the data set is not too big it will give us better prediction\n",
    "\n",
    "# input shape is the vocabulary count used for the reviews (10,000 words)\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train_padded1 = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=1000)\n",
    "x_test_padded1  = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=1000)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1000, input_shape=(1000,)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(512, input_shape=(1000,)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(256, input_shape=(1000,)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(num_classes))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Model Compilation\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_history = model.fit(\n",
    "    x_train_padded1,\n",
    "    y_train_sparse,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test_padded1, y_test_sparse),\n",
    "    verbose=1)\n",
    "\n",
    "scores = model.evaluate(x_test_padded1, y_test_sparse, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 512\n",
    "max_review_length = 1000\n",
    "\n",
    "model1 = keras.Sequential()\n",
    "model1.add(keras.layers.Embedding(vocab_size, embedding_vecor_length, input_length=max_review_length))\n",
    "model1.add(keras.layers.LSTM(100, return_sequences=True))\n",
    "model1.add(keras.layers.LSTM(100, return_sequences=True))\n",
    "model1.add(keras.layers.LSTM(100))\n",
    "model1.add(keras.layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "# optimizer\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# Model Compilation\n",
    "model1.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_history1 = model1.fit(\n",
    "    x_train_padded,\n",
    "    y_train_sparse,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test_padded, y_test_sparse),\n",
    "    verbose=1)\n",
    "\n",
    "scores1 = model1.evaluate(x_test_padded, y_test_sparse, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 512\n",
    "max_review_length = 1000\n",
    "\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Embedding(vocab_size, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(keras.layers.GlobalAveragePooling1D())\n",
    "model2.add(keras.layers.Dense(500, activation = 'relu'))\n",
    "model2.add(keras.layers.Dropout(0.2))\n",
    "model2.add(keras.layers.Dense(500, activation = 'relu'))\n",
    "model2.add(keras.layers.Dropout(0.2))\n",
    "model2.add(keras.layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "# Model Compilation\n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_history2 = model2.fit(\n",
    "    x_train_padded,\n",
    "    y_train_sparse,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test_padded, y_test_sparse),\n",
    "    verbose=1)\n",
    "\n",
    "scores2 = model2.evaluate(x_test_padded, y_test_sparse, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.evaluate(x_test_padded, y_test_sparse, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model2.predict(x_test_padded)\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_pred = pd.Series(y_pred, name='Predicted')\n",
    "y_test = pd.Series(y_test, name='Actual')\n",
    "df_confusion  = pd.crosstab(y_test,y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "#print(df_confusion)\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.xlabel('xlabel', fontsize=18)\n",
    "plt.ylabel('ylabel', fontsize=18)\n",
    "plt.title('Confusion Matrix',fontsize=20)\n",
    "sns.heatmap(df_confusion, annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "- It is possible to see that this artificial recurrent neural network (RNN) -> **Long short term memory** has a particularly great effect on the accuracy of this classification model, given by the fact that unlike standard feed forward neural networks, LSTM has feedback connections. It cannot only process single data points (such as images), but also entire sequences of data. The advantage of is clearly seen after using a standard deep neuron network model and see that the accuracy did not change from epoch to epoch.\n",
    "\n",
    "\n",
    "- As discussed during the class tuning the hyperparameters and choosing the right was mentioned to be a *dark art* and it is possible to see why in this example. Even though choosing the right hyperparameters is critical, besides some state of the art methods that are being developed right now, the best approach would be to talk to experts on the problem you are working on and on the Deep learning algorithm that is being used and do some try and error testing different scenarios.\n",
    "\n",
    "\n",
    "- Another thing that is possible to see (again) is that working on this specialized Deep Learning algorithms requires some knowledge of the kind of problem that we are working on as well as a very good understanding of the data available and the prediction that is wanted. Therefore working with the subject expert matter is a critical thing all the way thru a ML project.\n",
    "\n",
    "\n",
    "- Even though in this case the LSTM model got a better result for the classification problem we were working on compared just with a standard deep neural network, it was significantly more computationally expensive compared to the previous model, and also for this particular case there was another model available that was able to provide a better (more accurate) solution using way less computer power. Which makes clear that it is important to work with experts all the way thru the ML project and explore different options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
