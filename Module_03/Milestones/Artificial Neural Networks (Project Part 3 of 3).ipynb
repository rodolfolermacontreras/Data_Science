{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (Project Part 3/3)\n",
    "\n",
    "## Author - Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "Continue from Milestone 2 to build a simple neural netwok, a DNN & a RNN model and compare performance.\n",
    "\n",
    "# Abstract:\n",
    "\n",
    "The capstone project focuses on diaper manufacturing quality. Generally, to ensure or predict quality, a diaper manufacturer need s to monitor every step of the manufacturing process with sensors such as heat sensors, glue sensors, glue level, etc.\n",
    "For this capstone project, we will use the SECOM manufacturing Data Set from the UCI Machine Learning Repository.\n",
    "\n",
    "The analysis is is divided the following way:\n",
    "\n",
    "### Summary from Milestone 01 & Milestone 02\n",
    "\n",
    "\n",
    "### Data Formatting\n",
    "- **Merging Data**\n",
    "- **Missing Data**\n",
    "- **Repeated Values (Columns with the same value)**\n",
    "\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "### Analysis\n",
    "- **Normalizing features from 0-1**\n",
    "- **Split Data and Class Imbalance**\n",
    "    - Split Data\n",
    "    - Class Imbalance\n",
    "- **Simple Neural Network**\n",
    "- **Deep Neural Network**\n",
    "- **Recurrent Neural Network**\n",
    "\n",
    "### Summary of Models Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary from Milestone 01 & 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 01\n",
    "\n",
    "- For this analysis we initially had 590 features + date and the target variable (quality of the diaper as either good or bad). After some cleaning (removing columns with a lot of missing values and columns with the same value repeated in all rows) we ended up with a data frame with 538 features + data + target variable.\n",
    "\n",
    "\n",
    "- In this case it seems that the `Wrapper` feature selection method is the one offering a better selection to model the quality of the diapers by means of the manufacturing process.\n",
    "\n",
    "\n",
    "- The class imbalance problem was solved my means of re-sampling.\n",
    "\n",
    "| Target | -1 | 1294 |\n",
    "| --- | --- | --- |\n",
    "| Target | 1 | 99 |\n",
    "\n",
    "## Milestone 02\n",
    "\n",
    "- From this analysis the best model was the one from the `Random Forest` with the following performance values:\n",
    "\n",
    "| Performance | Value |\n",
    "| -- | --|\n",
    "| Accuracy | 0.9713 |\n",
    "| Error Rate | 0.0287 |\n",
    "| Precision | 0.9922 |\n",
    "| Recall | 0.9769|\n",
    "| F1 Score | 0.9845 |\n",
    "\n",
    "- The best hyperparameters for this model after a random grid search were:\n",
    "- `n_estimators`: 70\n",
    "- `min_samples_split`: 4\n",
    "- `min_samples_lead`:  2\n",
    "- `max_depth`: 10\n",
    "- `criterion`: gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load File\n",
    "filename = 'secom.data'\n",
    "filename2 = 'secom_labels.data'\n",
    "data = pd.read_csv(filename,header=None, sep = '\\s+')\n",
    "others = pd.read_csv(filename2,header=None,sep = '\\s+')\n",
    "\n",
    "others.columns = ['target', 'date']\n",
    "df = pd.concat([others,data], axis = 1)\n",
    "df['date']= pd.to_datetime(df['date']) #date to datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = []\n",
    "my_list = df.columns.values.tolist()\n",
    "y = df.shape\n",
    "for i in my_list:\n",
    "    x = df[i].isnull().sum()\n",
    "    missing_values.append(x)\n",
    "    \n",
    "columns_missing = dict(zip(my_list, missing_values))\n",
    "a = sorted(columns_missing.items(), key=lambda x: x[1], reverse = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before if more than 15% of data is missing in a column that column would be dropped from this data set at this moment. For other columns with missing data is present depending on how much data is missing is going to be either dropped or imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns_names = []\n",
    "for key, value in columns_missing.items():\n",
    "    if value > 240: #15% of the data\n",
    "        x = key\n",
    "        missing_columns_names.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those columns would be eliminated from the data set, since most of the values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(missing_columns_names, axis = 1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis since removing missing data would only take 10% of the available data, those raws are going to be remove. If by removing missing data more than 15% were to be removed other methods would have been explored as: substitution.\n",
    "Even at this moment having 540 variables/features is to much to explore and obtain meaninful results, therefore more work has to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Values (Columns with the same value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_repeated = []\n",
    "repeated = []\n",
    "updated_columns = df.columns.values.tolist()\n",
    "y = df.shape\n",
    "for i in updated_columns[2:]:\n",
    "    x = df[i].std()\n",
    "    if x == 0:\n",
    "        repeated.append(i)\n",
    "    else:\n",
    "        not_repeated.append(i)\n",
    "df = df.drop(repeated, axis = 1)\n",
    "\n",
    "categorical = []\n",
    "numerical = []\n",
    "updated_columns = df.columns.values.tolist()\n",
    "y = df.shape\n",
    "for i in updated_columns:\n",
    "    x = df[i].dtypes\n",
    "    if x == object:\n",
    "        categorical.append(i)\n",
    "    else:\n",
    "        numerical.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = df.iloc[:,0].tolist()\n",
    "binomial = []\n",
    "for i in range(len(tr)):\n",
    "    b = tr[i]\n",
    "    if b < 0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = 1\n",
    "    binomial.append(n)\n",
    "df['target_binomial'] = binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>577</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>target_binomial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>2008-07-19 17:53:00</td>\n",
       "      <td>2946.25</td>\n",
       "      <td>2432.84</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5287</td>\n",
       "      <td>...</td>\n",
       "      <td>13.7755</td>\n",
       "      <td>0.4949</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>3.8276</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>44.0077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 425 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                date        0        1          2          3       4  \\\n",
       "1      -1 2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294   \n",
       "2       1 2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102   \n",
       "3      -1 2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204   \n",
       "4      -1 2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334   \n",
       "5      -1 2008-07-19 17:53:00  2946.25  2432.84  2233.3667  1326.5200  1.5334   \n",
       "\n",
       "          6       7       8  ...      577     582     583     584      585  \\\n",
       "1  102.3433  0.1247  1.4966  ...  10.9003  0.5019  0.0223  0.0055   4.4447   \n",
       "2   95.4878  0.1241  1.4436  ...   9.2721  0.4958  0.0157  0.0039   3.1745   \n",
       "3  104.2367  0.1217  1.4882  ...   8.5831  0.4990  0.0103  0.0025   2.0544   \n",
       "4  100.3967  0.1235  1.5031  ...  10.9698  0.4800  0.4766  0.1045  99.3032   \n",
       "5  100.3967  0.1235  1.5287  ...  13.7755  0.4949  0.0189  0.0044   3.8276   \n",
       "\n",
       "      586     587     588       589  target_binomial  \n",
       "1  0.0096  0.0201  0.0060  208.2045                0  \n",
       "2  0.0584  0.0484  0.0148   82.8602                1  \n",
       "3  0.0202  0.0149  0.0044   73.8432                0  \n",
       "4  0.0202  0.0149  0.0044   73.8432                0  \n",
       "5  0.0342  0.0151  0.0052   44.0077                0  \n",
       "\n",
       "[5 rows x 425 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thru the code above the data was checked to see if there was any variable that could be considered as categorical (also this helped to checked any column with incorrect values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "From Milestone 01 it was seen that the best Features where does coming from the `Wrapper` Method. For more details on this comparison see **Milestone 01** analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass n_features_to_select=50 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator, 50, step=1)\n",
    "\n",
    "df_numeric = df[numerical]\n",
    "df_numeric = df_numeric.drop('date', axis = 1)\n",
    "\n",
    "df_numeric_copy = df_numeric.copy()\n",
    "\n",
    "target = df_numeric.iloc[:,0]#This is the selection of the Target Variables\n",
    "df_updated = df_numeric_copy.drop('target', axis='columns')\n",
    "\n",
    "lol = df_updated.values.tolist()\n",
    "selector = selector.fit(lol, target)\n",
    "\n",
    "boolean_values = selector.support_\n",
    "ranking_values = selector.ranking_\n",
    "wrapper_columns = df_updated.columns.values.tolist()\n",
    "\n",
    "#Selection of the features with ranking 1\n",
    "selected_features = []\n",
    "unselected_features = []\n",
    "for i in range(len(wrapper_columns)):\n",
    "    w = boolean_values[i]\n",
    "    if w == True:\n",
    "        b = wrapper_columns[i]\n",
    "        selected_features.append(b)\n",
    "    else:\n",
    "        v = wrapper_columns[i]\n",
    "        unselected_features.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>34</th>\n",
       "      <th>36</th>\n",
       "      <th>43</th>\n",
       "      <th>45</th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>53</th>\n",
       "      <th>...</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>377</th>\n",
       "      <th>387</th>\n",
       "      <th>392</th>\n",
       "      <th>446</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.5470</td>\n",
       "      <td>9.2599</td>\n",
       "      <td>191.2872</td>\n",
       "      <td>50.6596</td>\n",
       "      <td>49.3404</td>\n",
       "      <td>352.2445</td>\n",
       "      <td>133.1727</td>\n",
       "      <td>145.8445</td>\n",
       "      <td>631.2618</td>\n",
       "      <td>4.590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>1.5683</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202.0179</td>\n",
       "      <td>9.3144</td>\n",
       "      <td>192.7035</td>\n",
       "      <td>50.1530</td>\n",
       "      <td>49.8470</td>\n",
       "      <td>364.3782</td>\n",
       "      <td>131.8027</td>\n",
       "      <td>141.0845</td>\n",
       "      <td>637.2655</td>\n",
       "      <td>4.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>1.4698</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201.8482</td>\n",
       "      <td>9.6924</td>\n",
       "      <td>192.1557</td>\n",
       "      <td>50.5100</td>\n",
       "      <td>49.4900</td>\n",
       "      <td>363.0273</td>\n",
       "      <td>131.8027</td>\n",
       "      <td>142.5427</td>\n",
       "      <td>637.3727</td>\n",
       "      <td>4.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201.9424</td>\n",
       "      <td>10.3387</td>\n",
       "      <td>191.6037</td>\n",
       "      <td>50.2480</td>\n",
       "      <td>49.7520</td>\n",
       "      <td>353.3400</td>\n",
       "      <td>176.3136</td>\n",
       "      <td>138.0882</td>\n",
       "      <td>667.7418</td>\n",
       "      <td>4.624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>1.2524</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200.4720</td>\n",
       "      <td>9.2441</td>\n",
       "      <td>191.2280</td>\n",
       "      <td>50.5795</td>\n",
       "      <td>49.4205</td>\n",
       "      <td>360.2873</td>\n",
       "      <td>142.2591</td>\n",
       "      <td>137.6473</td>\n",
       "      <td>640.1936</td>\n",
       "      <td>4.636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        12       16        18       34       36        43        45   \\\n",
       "1  200.5470   9.2599  191.2872  50.6596  49.3404  352.2445  133.1727   \n",
       "2  202.0179   9.3144  192.7035  50.1530  49.8470  364.3782  131.8027   \n",
       "3  201.8482   9.6924  192.1557  50.5100  49.4900  363.0273  131.8027   \n",
       "4  201.9424  10.3387  191.6037  50.2480  49.7520  353.3400  176.3136   \n",
       "5  200.4720   9.2441  191.2280  50.5795  49.4205  360.2873  142.2591   \n",
       "\n",
       "        48        50     53   ...     365     366     367     368     377  \\\n",
       "1  145.8445  631.2618  4.590  ...  0.0053  0.0059  0.0081  0.0033  0.0013   \n",
       "2  141.0845  637.2655  4.486  ...  0.0054  0.0043  0.0030  0.0037  0.0015   \n",
       "3  142.5427  637.3727  4.486  ...  0.0046  0.0049  0.0028  0.0034  0.0014   \n",
       "4  138.0882  667.7418  4.624  ...  0.0063  0.0077  0.0052  0.0027  0.0012   \n",
       "5  137.6473  640.1936  4.636  ...  0.0095  0.0048  0.0033  0.0027  0.0011   \n",
       "\n",
       "   387     392     446     543     544  \n",
       "1  0.0  0.0027  1.5683  0.0078  0.0026  \n",
       "2  0.0  0.0020  1.4698  0.0078  0.0026  \n",
       "3  0.0  0.0023  1.3141  0.0078  0.0026  \n",
       "4  0.0  0.0077  1.2524  0.0078  0.0026  \n",
       "5  0.0  0.0081  1.1174  0.0078  0.0026  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[selected_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data and Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "tr = df.iloc[:,0].tolist()\n",
    "binomial = []\n",
    "for i in range(len(tr)):\n",
    "    b = tr[i]\n",
    "    if b < 0:\n",
    "        n = 0\n",
    "    else:\n",
    "        n = 1\n",
    "    binomial.append(n)\n",
    "df['target_binomial'] = binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing features from 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label = df['target_binomial']\n",
    "variables = df[selected_features]\n",
    "\n",
    "x = variables.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "variables_scaled = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding the variables to explore the normalization of the parameters as an input for the LSTM\n",
    "round_variables = variables_scaled*100\n",
    "variables_round = round_variables.round()\n",
    "variables_round = variables_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data for variables with high mi value\n",
    "X_train, X_test, y_train, y_test = train_test_split(variables_scaled, label, test_size=0.30, random_state=42)\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(variables_round, label, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "variables_sm, label_sm = sm.fit_resample(variables_scaled, label)\n",
    "variables_sm_rnn, label_sm_rnn = sm.fit_resample(variables_round, label)\n",
    "\n",
    "X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(variables_sm, label_sm, test_size=0.30, random_state=42)\n",
    "X_train_sm_rnn, X_test_sm_rnn, y_train_sm_rnn, y_test_sm_rnn = train_test_split(variables_sm_rnn, label_sm_rnn, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary the variables that are going to be input to the model are:\n",
    "- Training:\n",
    "    - `X_train_sm`\n",
    "    - `y_train_sm`\n",
    "    \n",
    "    \n",
    "- Testing:\n",
    "    - `X_test`\n",
    "    - `y_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "During Assignment 08 a simple and multiyer neural network was created from scratch, but in this case a model from the **scikit-learn** library will be used. Also it is important to notice that an ANN with no hidde layers would be basically a multi regression problem, for that reason the LogisticRegressioCV module would be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Some hyperparamaters:\n",
    "fit_inter = True #this is the Bias parameter\n",
    "max_iterations = 1000 #this is the number of epochs\n",
    "initial_weights = 'balanced'\n",
    "\n",
    "snn = LogisticRegressionCV(cv=5, \n",
    "                           fit_intercept = fit_inter,\n",
    "                           max_iter = max_iterations, \n",
    "                           solver='lbfgs',\n",
    "                           class_weight = initial_weights,\n",
    "                           random_state=0).fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing of the data\n",
    "y_snn_pred = snn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Neural Network (no hidden layers) accuracy: 69.85645933014354%\n"
     ]
    }
   ],
   "source": [
    "# Generate an accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "snn_accuracy = accuracy_score(y_test, y_snn_pred)*100\n",
    "print(\"Simple Neural Network (no hidden layers) accuracy: {}%\".format(snn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the noted results, a simple neural network model provided a fairly good Accuracy value. Compared with the previous models from Milestone 02: `Decision Tree`, `Random Forest` & `SVC` this model was not able to obtain the high Accuracy values from previous analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small comparison with model from scratch from Assignment 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a numerically stable logistic s-shaped definition to call\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))\n",
    "    \n",
    "# define the dimentions and set the weights to random numbers\n",
    "def init_parameters(dim1, dim2,std=1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2]))\n",
    "    \n",
    "# Single layer network: Forward Prop\n",
    "def fwd_prop(W1,bias,X):\n",
    "    Z1 = np.dot(W1,X) + bias# dot product of the weights and X + bias\n",
    "    A1 = sigmoid(Z1)# Uses sigmoid to create a predicted vector\n",
    "    return(A1)\n",
    "\n",
    "#Single layer network: Backprop\n",
    "def back_prop(A1,W1,bias,X,Y):\n",
    "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
    "    # Cross entropy loss function\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1))# cost of error\n",
    "    dZ1 = A1 - Y# subtract actual from pred weights\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)# calc new weight vector\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)# calc new bias vector\n",
    "    grads ={\"dW1\": dW1, \"dB1\":dBias}# Weight and bias vectors after backprop\n",
    "    return(grads,cost)\n",
    "\n",
    "def run_grad_desc(num_epochs,learning_rate,X,Y,n_1):\n",
    "    n_0, m = np.shape(X)\n",
    "    W1 = init_parameters(n_1, n_0, True)\n",
    "    B1 = init_parameters(n_1,1, True)\n",
    "    loss_array = np.ones([num_epochs])*np.nan# resets the loss_array to NaNs\n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = fwd_prop(W1,B1,X)# get predicted vector\n",
    "        grads,cost = back_prop(A1,W1,B1,X,Y)# get gradient and the cost from BP \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]# update weight vector LR*gradient*[BP weights]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]# update bias LR*gradient[BP bias]\n",
    "        loss_array[i] = cost# loss array gets cross ent values\n",
    "        parameter = {\"W1\":W1,\"B1\":B1}# assign \n",
    "    return(parameter,loss_array, A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the variables in the right format for the model from scratch\n",
    "#Training Data\n",
    "y_train_sm_df = pd.DataFrame(y_train_sm)\n",
    "X_train_sm_df = pd.DataFrame(X_train_sm)\n",
    "X = np.array(X_train_sm_df)\n",
    "X = X.T\n",
    "Y = np.array(y_train_sm_df)\n",
    "\n",
    "#Testing Data\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "K = np.array(X_test_df)\n",
    "K = K.T\n",
    "L = np.array(y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with no hidden layers\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "params, loss_array, prediction = run_grad_desc(num_epochs,\n",
    "                                               learning_rate,\n",
    "                                               X, \n",
    "                                               Y,\n",
    "                                               n_1= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = fwd_prop(params['W1'][:len(L)],params['B1'][:len(L)],K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.12315971183018%\n"
     ]
    }
   ],
   "source": [
    "delta = []\n",
    "test_pred = prediction_test[1]\n",
    "for i in range(len(prediction_test[1])):\n",
    "    d = abs(test_pred[i] - L[i])\n",
    "    delta.append(d)\n",
    "\n",
    "total_delta = sum(delta)\n",
    "nn0_accuracy = (1 - total_delta[0]/len(L))*100\n",
    "print('Accuracy: ' + str(nn0_accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a quick comparison (not really an exhaustive comparison) it seems to be that with 1000 Epochs the model from scratch simulating a simple neural network with no hidden layers and just 1 preceptor get us to a better Accuracy value on the testing data set, from 69% to 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Some hyperparamaters:\n",
    "hidden = 50\n",
    "act = 'relu'\n",
    "opt = 'adam'\n",
    "lr = 'adaptive'\n",
    "lr_value = 0.01\n",
    "epoch = 1000\n",
    "\n",
    "dnn = MLPClassifier(hidden_layer_sizes = hidden,\n",
    "                    activation = act,\n",
    "                    solver = opt,\n",
    "                    learning_rate = lr,\n",
    "                    learning_rate_init = lr_value,\n",
    "                    max_iter = epoch,\n",
    "                    random_state=1).fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing of the data\n",
    "y_dnn_pred = dnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network accuracy: 88.27751196172248%\n"
     ]
    }
   ],
   "source": [
    "dnn_accuracy = accuracy_score(y_test, y_dnn_pred)*100\n",
    "print(\"Deep Neural Network accuracy: {}%\".format(dnn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Deep Neural Network will be presented here\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_dim = 50, activation='relu'), \n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['BinaryAccuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25d1f76d898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sm, y_train_sm, verbose=0, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step - loss: 0.1125 - binary_accuracy: 0.9617\n",
      "Keras Deep Neural Network accuracy: 96.17224931716919%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "keras_dnn_accuracy = test_accuracy * 100\n",
    "print(\"Keras Deep Neural Network accuracy: {}%\".format(keras_dnn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see that the Keras model performed better than the MPL classifier from sklearn having 1000 epochs in both cases, but also it is possible to see that the computer resources used were pretty different as well. The Keras model consumed more resources than the sklearn model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the input for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LSTM Model\n",
    "def sequence_LSTM(df):\n",
    "    sequence = []\n",
    "    for i in range(len(df[1])):\n",
    "        rows = df.iloc[i]\n",
    "        rows1 = rows.tolist()\n",
    "        sequence.append(rows1)\n",
    "    return sequence\n",
    "        \n",
    "X_test_LSTM = sequence_LSTM(X_test_rnn)\n",
    "X_train_sm_LSTM = sequence_LSTM(X_train_sm_rnn)\n",
    "\n",
    "X_train_sm_LSTM = np.array(X_train)\n",
    "X_test_LSTM = np.array(X_test)\n",
    "\n",
    "train_data = X_train_sm_LSTM.reshape(975, 1, 50)\n",
    "test_data = X_test_LSTM.reshape(418, 1, 50)\n",
    "\n",
    "y_train_data = np.array(y_train)\n",
    "y_test_data = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 975, 100)          60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 975, 100)          80400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 221,301\n",
      "Trainable params: 221,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 975, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 975, 50), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 50).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 975, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 975, 50), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 50).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 975, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 975, 50), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 50).\n",
      "LSTM Accuracy: 93.06%\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.Sequential()\n",
    "model1.add(keras.layers.LSTM(100, input_shape = (975,50), return_sequences=True))\n",
    "model1.add(keras.layers.LSTM(100, return_sequences=True))\n",
    "model1.add(keras.layers.LSTM(100))\n",
    "model1.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "# Model Compilation\n",
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['BinaryAccuracy'])\n",
    "\n",
    "model1.fit(\n",
    "    train_data,\n",
    "    y_train_data,\n",
    "    epochs=50,\n",
    "    validation_data=(test_data, y_test_data),\n",
    "    verbose=0)\n",
    "\n",
    "scores1 = model1.evaluate(test_data, y_test_data, verbose=0)\n",
    "keras_rnn_accuracy = scores1[1]*100\n",
    "print(\"LSTM Accuracy: %.2f%%\" % (keras_rnn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance = [snn_accuracy,nn0_accuracy,dnn_accuracy,keras_dnn_accuracy,keras_rnn_accuracy ]\n",
    "y = ['snn','snn_scratch','sklearn_dnn','keras_dnn', 'keras_rnn']\n",
    "fig, ax = plt.subplots(figsize = (12,6))    \n",
    "ax.bar(y, performance, color = ['lightseagreen', 'tomato', 'darkmagenta', 'crimson', 'olive'])\n",
    "plt.title('Performance of each Model', fontsize = 25)\n",
    "plt.ylabel('Accuracy',fontsize = 15)  \n",
    "plt.tick_params(axis=\"x\", labelsize=15)\n",
    "plt.tick_params(axis=\"y\", labelsize=15)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is seen in the plot above the best 2 options is a deep learning model with 2 hidden layers and an RNN model using LTSM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of findings\n",
    "\n",
    "- As seen in the analysis it seems that the best 2 options where the deep learning model and the LSTM model, both using keras, with 96% and 93% respectively. Something to take into account is that from Milestone 2 Random Forest got a 97% and it is to an extend a white box algorithm which gives the advantage of providing direct feedback on what features are more relevant to the prediction of the quality of the diaper.\n",
    "\n",
    "\n",
    "- Deep learning models even though powerful, it should be treated and done with a lot of care. Meaning there is a lot of that needs to be consider before jumping to use a more complex model that potentially could have been done with another machine learning algorithm that would provide a better advantage. Among other things: expertise on the problem that wants to be solved, expertise in the kind of deep learning model that wants to be used, clear understanding of the assumptions and limitations of the problem and model and maybe more importantly a really good idea of the potential bias intrinsic to data that comes from an imperfect word.\n",
    "\n",
    "\n",
    "- Another important point to notice is the selection of the best hyperparameters, a good way to start is by using best practices in regards the kind of problem that is being analyzed and talking to experts on the matter. Once a good guess of the best options is collected, then a grid search (or random search) around that good guess might be a great way to obtain the best hyperparameters for the mode, which is as important as any part of the analysis and machine learning model. If it is not possible to obtain a good guess, just an exploration by using grid or random search can provide a very good estimate of these parameters.\n",
    "\n",
    "\n",
    "- Some recommendations for the manufacturer would be: \n",
    "\n",
    "    - *Keep getting data, the more data we have the better models it would be able to develop.*\n",
    "    - *That leads me to the 2nd point, the other suggestion is to make sure the check their sensors and data periodically to make sure no false information is being input to the model. This could be done by working on periodic checks on the instrumentation (trends and direct comparisons with previous/calibrated states).*\n",
    "    - *The list of the most relevant 50 sensors can be found in this analysis, which are the ones that we have to ensure receive proper maintenance. No to say that the other sensors does not matter, but the ones included in this analysis are the ones that provide better inside to the model in relation to the quality of the diapers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Request:\n",
    "\n",
    "1. What is the best way or the most practice way of getting hyperparameters for a deep learning model,  especially when using tensorflow and keras?\n",
    "\n",
    "\n",
    "2. What is the best (or more common) way of knowing the number of layer and neurons for a problem like this one? Knowing that exploring all the combinations is very computationally expensive how do you sample a subset where the probability of higher accuracy is given?\n",
    "\n",
    "\n",
    "3. In which case make sense to add other kind of deep layers to the model and not only dense layers but others?\n",
    "\n",
    "\n",
    "4. Deep learning being a black box algorithm what feedback usually can be obtain from the model? what other performance parameters could be significant for this kind of manufacturing problems?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
