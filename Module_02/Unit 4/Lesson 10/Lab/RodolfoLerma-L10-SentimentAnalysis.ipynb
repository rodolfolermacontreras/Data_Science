{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10 - Introduction to Text Analytics\n",
    "\n",
    "## Author - Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Section 10A`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Are Everywhere\n",
    "\n",
    "Raw text data is an unstructured and ubiquitous type of data. Most of the world’s data is unstructured. Volumes of unstructured data, including text, are growing much faster than structured data. There are many industry estimates for the fraction of all data which is unstructured. A few from the last 8 years include:   \n",
    "- 2009 HP Survey: 70%\n",
    "- Gartner: 80%\n",
    "- Teradata: 85%\n",
    "- But, **Beware of industry estimates!!**\n",
    "\n",
    "How much text data are we talking about here? In a few years time, Twitter has more text data recorded than all that has been written in print in the history of mankind. (http://www.internetlivestats.com/twitter-statistics/)\n",
    "\n",
    "### Applications of Text Analytics\n",
    "\n",
    "Given the ubiquity and volume of text data, it is not surprising that numerous powerful applications which exploit text analytics are appearing. A few of these applications are listed below.\n",
    "\n",
    "- Intelligent applications\n",
    "  - Assistants\n",
    "  - Chat bots\n",
    "- Classification\n",
    "  - Sentiment analysis\n",
    "  - SPAM detection\n",
    "- Speech recognition\n",
    "- Search\n",
    "- Information retrieval\n",
    "- Legal discovery\n",
    "\n",
    "### Analysis of Text Data\n",
    "\n",
    "In this tutorial we investigate three areas of text analytics. The following three sections cover these topics.\n",
    "\n",
    "- Preparing text for analysis.\n",
    "- Classification of text and sentiment analysis.\n",
    "- Topic Models for document classification and retrieval. \n",
    "\n",
    "We start by loading the necessary libraries for this jupyter notebook script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import csv\n",
    "#import lda\n",
    "import editdistance\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialty libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import Counter\n",
    "import argparse\n",
    "import pprint\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glove import Glove\n",
    "# from glove import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ly266e\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ly266e\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# If you get an SSL-certificate error, and you are on a MAC then you may have to navigate to: application/python3/ and\n",
    "# run/double-click on the command 'install certificates'.  Then try this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing  text data\n",
    "\n",
    "By its very nature, text data comes unstructured and poorly organized for analysis. Typically multiple steps are required to process text into a form suitable for analysis. You can think of this process as transforming the unstructured data into a structured set of features. \n",
    "\n",
    "Steps covered in this tutorial include the following:\n",
    "\n",
    "- Organize text documents into a corpus\n",
    "- Normalize the text to remove unneeded content\n",
    "  - Tokenize text\n",
    "  - Clean text\n",
    "- Create term document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text analysis methods\n",
    "\n",
    "There are a great many approaches which have been tried for text analytics and natural language processing (NLP). We only mention a few below. \n",
    "\n",
    "- The **bag of words model** is a simple widely used and surprisingly effective model for analysis of text data. The BOW model uses only on the frequency of the words in the document and order of the words is not considered. Despite these seemingly ridiculous assumptions, the model works well in many cases. \n",
    "  - The BOW model assumes **exchangeability** of words. \n",
    "  - The end product of applying the BOW model is a term-document or document-term matrix. The tdm, or dtm is a structured representation of word frequency by document. \n",
    "  - The tdm or dtm can be used for classification if labels are available or clustering for unsupervised learning. \n",
    "- Other powerful models are the **word to vec** and **doc to vec** models. Word to vec, uses a neural network model to determine similarity between words. These models are beyond the scope of this tutorial. You can find a good introduction to this model in the [article by Rong](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- Another widely used model is of **Part of Speech (PoS) Tagging**. PoS tagging attempts to label or annotate words in a corpus (e.g. a collection of documents) as, say nouns, verbs, pronouns, etc. PoS tagging is beyond the  scope of this tutorial. The PoS tagger creates a tree of the relationship of words in say a sentence. One useful specialization of PoS tagging is named entity recognition, which attempts to find proper nouns. \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with quantifying text, we will first consider measuring a distance between text (strings).\n",
    "\n",
    "## Quantifying Text\n",
    "\n",
    "With data we can easily ask what is the distance between numeric data points. It is harder to answer that question with text data. There are ways to quantify the distance between different text but we have to be specific on how we measure the distance.\n",
    "\n",
    "### Measuring Text Distance\n",
    "\n",
    "Measuring the distance between words in a document is not as straight forward as it might seem. The choice of distance metric can have a significant effect on analytical results. This is particularly the case for unsupervised learning methods like cluster models. \n",
    "\n",
    "Let's look at a few of the commonly used distance metrics. \n",
    "\n",
    "**Hamming Distance**\n",
    "- Line up strings, count number of positions that are the different.\n",
    "- Assumes strings are of the same length.\n",
    "\n",
    "$$𝐻𝑎𝑚𝑚𝑖𝑛𝑔(101101, 100011)=3\\\\\n",
    "𝐻𝑎𝑚𝑚𝑖𝑛𝑔(𝑏𝑒𝑒𝑟,𝑏𝑒𝑎𝑟)=1$$\n",
    "\n",
    "**Levenshtein distance** also called the 'edit distance', measure the distance between two strings (insertion, deletion, substitution only):\n",
    "\n",
    "$$𝐿𝑒𝑣(𝑏𝑒𝑒𝑟,𝑏𝑒𝑎𝑟)=1\\\\\n",
    "𝐿𝑒𝑣(𝑏𝑎𝑛𝑎𝑛𝑎,𝑏𝑎𝑛)=3$$\n",
    "\n",
    "**Jaccard index** measures the size of intersection of characters divided by size of union of characters.\n",
    "\n",
    "$$J(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\\\\\n",
    "J(beer, bear) = 1 - \\frac{3}{4}\\\\\n",
    "J(bannana, ban) = 1 - \\frac{3}{3} \\leftarrow\\ This\\ is\\ a\\ problem$$\n",
    "\n",
    "**Weighted Jaccard Index** For each letter, calculate the minimum times it appears, $m_i$, and the maximum number of times it appears, $M_i$.\n",
    "\n",
    "$$J'(A, B) = 1 - \\frac{\\sum m_i}{\\sum M_i}\\\\\n",
    "J'(beer, bear) = 1 - \\frac{m_a + m_e + m_b + m_r}{M_a + M_e + M_b + M_r}\\\\\n",
    "J'(beer, bear) = 1 - \\frac{0 + 1 + 1 + 1}{1 + 1 + 2 + 1}=1-\\frac{3}{5} = \\frac{2}{5}\\\\\n",
    "J'(banana, ban) = 1 - \\frac{1 + 1 + 1}{3 + 1 + 2}=\\frac{1}{2}$$\n",
    "\n",
    "Next we show how to implement these distances with python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming(101101, 100011) = 3\n",
      "Hamming(bear, beer) = 1\n"
     ]
    }
   ],
   "source": [
    "# Hamming Distance\n",
    "def hamming_dist(text1, text2):\n",
    "    # For strings of equal length:\n",
    "    assert len(text1) == len(text2)\n",
    "    return sum(char1 != char2 for char1, char2 in zip(text1, text2))\n",
    "\n",
    "print('Hamming(101101, 100011) = {}'.format(hamming_dist('101101', '100011')))\n",
    "print('Hamming(bear, beer) = {}'.format(hamming_dist('bear', 'beer')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lev_Dist(bear, beer) = 1\n",
      "Lev_Dist(banana, ban) = 3\n"
     ]
    }
   ],
   "source": [
    "# Levenshtein Distance (edit distance)\n",
    "print('Lev_Dist(bear, beer) = {}'.format(editdistance.eval('bear', 'beer')))\n",
    "\n",
    "print('Lev_Dist(banana, ban) = {}'.format(editdistance.eval('banana', 'ban')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard(bear, beer) = 0.25\n",
      "Jaccard(banana, ban) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Jaccard Index Distance\n",
    "def jaccard_ix_dist(text1, text2):\n",
    "    set1 = set(list(text1))\n",
    "    set2 = set(list(text2))\n",
    "    return 1 - (len(set1.intersection(set2)) / float(len(set1.union(set2))))\n",
    "\n",
    "\n",
    "print('Jaccard(bear, beer) = {}'.format(jaccard_ix_dist('beer', 'bear')))\n",
    "\n",
    "# The following may be an issue...\n",
    "print('Jaccard(banana, ban) = {}'.format(jaccard_ix_dist('banana', 'ban')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Jaccard(bear, beer) = 0.4\n",
      "Weighted Jaccard(banana, ban) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Weighted Jaccard Index\n",
    "def weighted_jaccard_dist(text1, text2):\n",
    "    set1 = set(list(text1))\n",
    "    set2 = set(list(text2))\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    min_sum = 0\n",
    "    max_sum = 0\n",
    "    for char in union:\n",
    "        min_sum += min(text1.count(char), text2.count(char))\n",
    "        max_sum += max(text1.count(char), text2.count(char))\n",
    "    \n",
    "    return 1 - (min_sum / max_sum)\n",
    "\n",
    "print('Weighted Jaccard(bear, beer) = {}'.format(weighted_jaccard_dist('beer', 'bear')))\n",
    "\n",
    "print('Weighted Jaccard(banana, ban) = {}'.format(weighted_jaccard_dist('banana', 'ban')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 1\n",
    "What is the weighted jaccard distance between the phrase 'open the pod bay doors, hall' and 'Open the pod-bay doors Hall!'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'open the pod bay doors, hall'\n",
    "b = 'Open the pod-bay doors Hall!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Jaccard(a, b) = 0.25\n"
     ]
    }
   ],
   "source": [
    "print('Weighted Jaccard(a, b) = {}'.format(weighted_jaccard_dist(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Preprocessing\n",
    "\n",
    "The very first step to prepare text is to clean it.  We clean and normalize the text by performing various operations on the text. Some examples are as follows:\n",
    "\n",
    " - Lowercase the text.\n",
    " - Remove symbols and/or punctuation.\n",
    " - Remove numbers. May also replace all numbers with a numeric tag, for example `<NUM>` or similar. We may also consider replacing all dates with `<DATE>` or similarly use tags `<URL>`, `<PHONE>`, `<EMAIL>`, etc...\n",
    " - Strip extra white space. White space has many forms: space, newline, or tab. There are also other rarely used unicode specifications for other white space characters.\n",
    " - Remove all non-printable unicode characters.\n",
    " - Replace accent characters (For example: N̈ --> N).\n",
    " - Remove 'stop words'. Stop words are generally non-informative words like 'the, as, a, ours, etc...'\n",
    " - Stem words to similar endings. (For example: 'statistics', 'statistical', 'statistic' --> 'statisti')\n",
    " - Create bi-grams (or tri-grams, or N-grams, ...). (For example, 'out of date' --> 'out-of-date' or 'out_of_date')\n",
    " \n",
    "There are a few reasons to clean your text.  The primary reason is to reduce the potential vocabulary and increase the observations of specific words (or tokens).  When considering which of the above steps to apply in your text analysis or NLP project, consider how you want the problem at hand to treat different instances of tokens.  Ask yourself questions like \"Do I want to consider the words 'China' and 'china' to be different?\" In the prior example one is a country and the other is a plate. Or maybe numbers are important in our text. The answers to these questions determine which pre-processing steps to perform and is very problem specific.\n",
    "\n",
    "***\n",
    "**Note**: Be careful dealing with unicode characters. There are many editors and text viewers that only display printable characters but will not remove non-printable characters. Strange unicode characters can end up in data from users blindly copy/pasting text (with invisible unicode) into other text boxes.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered text: \n",
      "I <3 statistics $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitE!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning example\n",
    "horrible_tweet_text = 'I <3 statistics $\\ \\ $, it’s my \\u1072  $\\ \\ $    fAvoRitE!! 11!!!'\n",
    "print('Unfiltered text: \\n{}\\n'.format(horrible_tweet_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed non-ascii unicode: \n",
      "I <3 statistics $\\ \\ $, its my   $\\ \\ $    fAvoRitE!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove non-ascii unicode\n",
    "clean1 = ''.join([x for x in horrible_tweet_text if ord(x) < 128])\n",
    "print('Removed non-ascii unicode: \\n{}\\n'.format(clean1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased: \n",
      "i <3 statistics $\\ \\ $, its my   $\\ \\ $    favorite!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove uppercase\n",
    "clean2 = clean1.lower()\n",
    "print('Lowercased: \\n{}\\n'.format(clean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Punctuation: \n",
      "i 3 statistics    its my         favorite 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "clean3 = ''.join(char for char in clean2 if char not in exclude)\n",
    "print('No Punctuation: \\n{}\\n'.format(clean3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed numbers: \n",
      "i  statistics    its my         favorite \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers\n",
    "clean4 = re.sub(\"\\d+\", \"\", clean3)\n",
    "print('Removed numbers: \\n{}\\n'.format(clean4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripped extra whitespace: \n",
      "i statistics its my favorite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strip extra whitespace\n",
    "clean5 = ' '.join(clean4.split())\n",
    "print('Stripped extra whitespace: \\n{}\\n'.format(clean5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopwords like: ['i', 'me', 'my', 'myself', 'we'] ...\n",
      "statistics favorite\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "word_list = clean5.split(' ')\n",
    "clean_word_list = [word for word in word_list if word not in stopwords.words('english')]\n",
    "clean6 = ' '.join(clean_word_list)\n",
    "print('Removed stopwords like: {} ...'.format(stopwords.words('english')[0:5]))\n",
    "print(clean6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Words: \n",
      "statistic favorite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stem Words\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stemmed_words = [lmtzr.lemmatize(word) for word in clean_word_list]\n",
    "clean7 = ' '.join(stemmed_words)\n",
    "print('\\nStemmed Words: \\n{}\\n'.format(clean7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic favorite\n"
     ]
    }
   ],
   "source": [
    "# Create a function to do this\n",
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "step_list = ['remove_non_ascii', 'lowercase', 'remove_punctuation', 'remove_numbers',\n",
    "            'strip_whitespace', 'remove_stopwords', 'stem_words']\n",
    "print(preprocess(horrible_tweet_text, step_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 2\n",
    "\n",
    "What is the expected differences between the following commands?\n",
    "the world ``statistics``. One of them should have the statistics and the other statistic.\n",
    "\n",
    "And why are they different?\n",
    "Because of the order of cleaning and preprocessing. In the first case when the word is lemmatized includes the 200 end, therefore that is what it will remove. In the second case that ending was already removed and therefore can look for other endings to generalized the word, which in this case is the letter *s*.\n",
    "\n",
    "`my_string = 'We took statistics200'`\n",
    "\n",
    " 1. `preprocess(my_string, ['stem_words', 'remove_numbers'])`\n",
    " 2. `preprocess(my_string, ['remove_numbers', 'stem_words'])`\n",
    " \n",
    "Can you think of any other examples where the order of preprocessing text matters?\n",
    "- As seen in this example, every time we have a words with numbers (or the possibility that we might have those situations) \n",
    "- In general every time a word might be concatenated with other characters (including punctuation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = 'We took statistics200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We took statistics'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(my_string, ['stem_words', 'remove_numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We took statistic'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(my_string, ['remove_numbers', 'stem_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Section 10B`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preparation\n",
    "Unstructured text must be processed into a uniform set of features suitable for further analysis. In this section we will step through some of the commonly used methods for converting unstructured text into a form we can use for analysis. There are four steps we need to transform text into a set of features we can analyze.  \n",
    "\n",
    "- **Clean** the texts entries (see above section).\n",
    "- **Tokenize** the document.\n",
    "- **Normalize** the text. \n",
    "- Compute the **term-document matrix** or **document-term matrix**.\n",
    "\n",
    "### Tokenize Text\n",
    "\n",
    "As a first step in preparing text for analysis of a document is to **tokenize** the text. In general terms, tokenization is the process dividing raw text into words, symbols and other elements, known as **tokens**. A set of tokens from one or more documents is known as a **corpus**.\n",
    "\n",
    "As a first step in creating a corpus is reading the data set. This particular data set is comprised of 160,000 tweets. The sentiment of these tweets has been human labeled as positive or negative {0,4}. The code in the cell below reads the tweet text and sentiment. The sentiment is marked as {0,1} for positive and negative. Run this code to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text\n",
       "0                4  @elephantbird Hey dear, Happy Friday to You  A...\n",
       "1                4  Ughhh layin downnnn    Waiting for zeina to co...\n",
       "2                0  @greeniebach I reckon he'll play, even if he's...\n",
       "3                0              @vaLewee I know!  Saw it on the news!\n",
       "4                0  very sad that http://www.fabchannel.com/ has c..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Tweet dataset\n",
    "data_file = 'twitter_data.csv'\n",
    "tweet_df = pd.read_csv(data_file)\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text\n",
      "0                1  @elephantbird Hey dear, Happy Friday to You  A...\n",
      "1                1  Ughhh layin downnnn    Waiting for zeina to co...\n",
      "2                0  @greeniebach I reckon he'll play, even if he's...\n",
      "3                0              @vaLewee I know!  Saw it on the news!\n",
      "4                0  very sad that http://www.fabchannel.com/ has c...\n",
      "\n",
      "\n",
      "count    160000.000000\n",
      "mean          0.500000\n",
      "std           0.500002\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.500000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "Name: sentiment_label, dtype: float64\n",
      "\n",
      "\n",
      " Count of positives: 80000\n"
     ]
    }
   ],
   "source": [
    "# But sentiment is either '4' or '0'. We'll change that to '1' or '0' to indicate positive or negative sentiment.\n",
    "tweet_df.sentiment_label=tweet_df.sentiment_label.replace(4,1)\n",
    "\n",
    "# Check the Data frame again\n",
    "print(tweet_df.head())\n",
    "\n",
    "print('\\n\\n{}'.format(tweet_df['sentiment_label'].describe()))\n",
    "\n",
    "print('\\n\\n Count of positives: {}'.format(np.sum(tweet_df['sentiment_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a copy of the tweets as list for use later\n",
    "tweet_data = tweet_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 3px; background-color: #f5f5f5; padding: 5px;\">\n",
    "The following code has been updated to reflect the data set location.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set read, we need to clean then tokenize the tweets.\n",
    "\n",
    "***\n",
    "**Note**: Becareful stemming on a large dataset. Stemming is the slowest procedure in the list and can really slow you down. For demonstration purposes, we will not perform stemming.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace']\n",
    "\n",
    "tweet_df['clean_tweet'] = tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "      <td>elephantbird hey dear happy friday to you alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "      <td>ughhh layin downnnn waiting for zeina to cook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "      <td>greeniebach i reckon hell play even if hes not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "      <td>valewee i know saw it on the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "      <td>very sad that httpwwwfabchannelcom has closed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text  \\\n",
       "0                1  @elephantbird Hey dear, Happy Friday to You  A...   \n",
       "1                1  Ughhh layin downnnn    Waiting for zeina to co...   \n",
       "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
       "3                0              @vaLewee I know!  Saw it on the news!   \n",
       "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  elephantbird hey dear happy friday to you alre...  \n",
       "1  ughhh layin downnnn waiting for zeina to cook ...  \n",
       "2  greeniebach i reckon hell play even if hes not...  \n",
       "3                  valewee i know saw it on the news  \n",
       "4  very sad that httpwwwfabchannelcom has closed ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Document Matrix (Quiz Point 1)\n",
    "\n",
    "Now that we have a corpus with some basic normalization applied, we can create a **term document matrix (tdm)** The tdm is a representation of **Bag of Words** model. The tdm has the following properties:\n",
    "\n",
    "- Frequencies of a given term  are in the rows. The **term frequencies (TF)** for each document are in the columns\n",
    "- The tdm is a sparse matrix, as most documents do not include many of the terms. Sparse matrix coding must be used for efficiency. \n",
    "- **Document term matrix (dtm)** is transpose\n",
    "- Using the distribution of a document’s TF or **term frequency inverse document  frequency (TF-IDF)** values a number of analyses can be performed, including:\n",
    "  - Characterize writing styles\n",
    "  - Comparing authors\n",
    "  - Determining original authors\n",
    "  - Finding plagiarism\n",
    "\n",
    "Let's look at an example of a tdm. The figure below shows a corpus of text documents on the left. This corpus is transformed into the term document matrix shown on the right. Notice that the matrix is sparse as any given document may not contain a term. Additionally, some terms may appear in the document multiple times. \n",
    "\n",
    "![](https://library.startlearninglabs.uw.edu/DATASCI410/img/tdm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12 distinct words.\n",
      "\n",
      "['fun', 'coding', 'learn', 'machines', 'learning', 'much', 'i', 'so', 'think', 'can', 'is', 'machine']\n",
      "[[1 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 1]\n",
      " [0 0 1 0]\n",
      " [1 1 0 1]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Document term matrix example\n",
    "example_texts = [\n",
    "    'machine learning is so much fun',\n",
    "    'i think learning is fun',\n",
    "    'machines can learn',\n",
    "    'i think coding is fun'\n",
    "]\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = set()\n",
    "for text in example_texts:\n",
    "    words = text.split(' ')\n",
    "    vocab.update(set(words))\n",
    "\n",
    "# fix the set as a list\n",
    "vocab_list = list(vocab)\n",
    "print('Vocabulary Size: {} distinct words.\\n'.format(len(vocab_list)))\n",
    "\n",
    "# initialize empty term-document matrix\n",
    "d_t_matrix = np.zeros((len(vocab), len(example_texts)), dtype=np.intc)\n",
    "for doc_ix_col, text in enumerate(example_texts):\n",
    "    text_words = text.split(' ')\n",
    "    row_ixs = [vocab_list.index(word) for word in text_words if word in vocab_list]\n",
    "    d_t_matrix[row_ixs, doc_ix_col] = 1\n",
    "\n",
    "print(vocab_list)\n",
    "print(d_t_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that with a large corpus, our term-document matrix will get very sparse very quickly.\n",
    "\n",
    "Numpy/Scipy has a great way to deal with sparse matrices.  We will store the occurrences with coordinates and values. This type of sparse matrix is called a `COO matrix`, or a COOrdinate matrix.\n",
    "\n",
    "Let's see how to do this for the tweet-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry: ['waiting', 'for', 'my', 'flight', 'to', 'tokyo', 'japan', 'im', 'not', 'looking', 'forward', 'hours', 'on', 'my', 'ass']\n"
     ]
    }
   ],
   "source": [
    "# Create a document storage matrix\n",
    "clean_texts = tweet_df['clean_tweet']\n",
    "docs = {}\n",
    "labels = []\n",
    "for ix, row in enumerate(clean_texts):\n",
    "    # Store the sentiment\n",
    "    labels = tweet_data[ix][0]\n",
    "    docs[ix] = row.split(' ')\n",
    "\n",
    "# See a sample\n",
    "print('Example entry: {}'.format(docs[np.random.choice(ix)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our tweet-vocabulary has 151670 distinct words.\n"
     ]
    }
   ],
   "source": [
    "# We want to keep track of how many unique words there are:\n",
    "num_nonzero = 0\n",
    "vocab = set()\n",
    "\n",
    "for word_list in docs.values():\n",
    "    unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "    vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "    num_nonzero += len(unique_terms) # add count of unique terms in this tweet\n",
    "\n",
    "doc_key_list = list(docs.keys())\n",
    "\n",
    "print('Our tweet-vocabulary has {} distinct words.'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert everything to a numpy array:\n",
    "doc_key_list = np.array(doc_key_list)\n",
    "vocab = np.array(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['wesaysummerrr' 'krissyb' 'camik' 'lawruhx' 'shisha']\n",
      "Sorted Vocab: ['a' 'aa' 'aaa' 'aaaa' 'aaaaa']\n"
     ]
    }
   ],
   "source": [
    "# We should keep track of how the vocab/term indices map to the matrix so that we can look them up later.\n",
    "vocab_sorter = np.argsort(vocab)\n",
    "\n",
    "print('Vocab: {}'.format(vocab[:5]))\n",
    "print('Sorted Vocab: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our sparse matrix:\n",
    "num_docs = len(doc_key_list)\n",
    "vocab_size = len(vocab)\n",
    "# A COO matrix is just a tuple of data, row indices, and column indices. Everything else is assumed to be zero.\n",
    "data = np.empty(num_nonzero, dtype=np.intc)     # all non-zero\n",
    "rows = np.empty(num_nonzero, dtype=np.intc)     # row index\n",
    "cols = np.empty(num_nonzero, dtype=np.intc)     # column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full term-document matrix (sparse), please wait!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "# go through all documents with their terms\n",
    "print('Computing full term-document matrix (sparse), please wait!')\n",
    "for doc_key, terms in docs.items():\n",
    "    # find indices to insert-into such that, if the corresponding elements were\n",
    "    # inserted before the indices, the order would be preserved\n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "    n_vals = len(uniq_indices)  # = number of unique terms\n",
    "    ix_end = ix + n_vals # Add count to index.\n",
    "\n",
    "    data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "    doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "    rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "\n",
    "    ix = ix_end  # resume with next document -> will add future data on the end.\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five terms alphabetically: ['a' 'aa' 'aaa' 'aaaa' 'aaaaa']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our sorted vocabulary again\n",
    "print('First five terms alphabetically: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we probably need to do some trimming, as the word 'aaaaa' probably doesn't occur often enough, and having 151,670 unique words may be too much.  We will address this later on. For now, let's create the sparse coordinate matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sparse matrix!\n",
    "doc_term_mat = coo_matrix((data, (rows, cols)), shape=(num_docs, vocab_size), dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab index of math : 25810\n",
      "\n",
      "1st document index containing said word: 171\n",
      "\n",
      "Tweet: [1, 'yayyy, i miss out on science and math tomoro!!  but then i have french in the morning. eugh, fair trade =__=']\n",
      "\n",
      "Document-Term Matrix entry: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's check to make sure!\n",
    "vocab_list = list(vocab)\n",
    "word_of_interest = 'math'\n",
    "vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# Find which tweets contain word:\n",
    "doc_ix_with_word = []\n",
    "for ix, row in enumerate(tweet_data): # Note on this line later.\n",
    "    if word_of_interest in row[1]:\n",
    "        doc_ix_with_word.append(ix)\n",
    "\n",
    "print('\\n1st document index containing said word: {}'.format(doc_ix_with_word[0]))\n",
    "print('\\nTweet: {}'.format(tweet_data[doc_ix_with_word[0]]))\n",
    "\n",
    "# Document - term matrix relevant entry:\n",
    "document_row = doc_ix_with_word[0]\n",
    "vocab_col = vocab_interesting_ix \n",
    "mat_entry = doc_term_mat.tocsr()[document_row, vocab_col]\n",
    "\n",
    "print('\\nDocument-Term Matrix entry: {}'.format(mat_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful with our usage of test-words and python's `in` function.  Imagine we have a tweet that has punctuation or the word is a sub-sequence of another word, then the `in` function would tag the document as true, but our cleaning/parsing would not have that as a separate word.\n",
    "\n",
    "You can see for yourself what is happening if you use the test word \"python\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 3\n",
    "Question: Why does using the 'in' command for the word 'python' come up with a count of zero in the matrix?\n",
    "\n",
    "The command `in` will look for sets of words where the word that I am looking for is included if not specified otherwise. For example under the `in` command any tweet where: python, pppython & pythonnnn exist are going to be included in the list. But when it comes to the spare matrix (after cleaning/parsing) that would potentially count that value (word) as a unique occurrence, therefore it will not find a match in the matrix for that particular word as both are doing something different and looking for different words: The first one is looking for the word python anywhere were it can be found regardless of what is around that word, while the other is looking for an exact occurrence of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab index of python : 101824\n",
      "\n",
      "1st document index containing said word: 14592\n",
      "\n",
      "Tweet: [1, \"omg, I just found my old vid of monty python's life of brian. it's brilliant, I just can't get enough stan today\"]\n",
      "\n",
      "Document-Term Matrix entry: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's check to make sure!\n",
    "vocab_list = list(vocab)\n",
    "word_of_interest = 'python'\n",
    "vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# Find which tweets contain word:\n",
    "doc_ix_with_word = []\n",
    "for ix, row in enumerate(tweet_data): # Note on this line later.\n",
    "    if word_of_interest in row[1]:\n",
    "        doc_ix_with_word.append(ix)\n",
    "\n",
    "print('\\n1st document index containing said word: {}'.format(doc_ix_with_word[0]))\n",
    "print('\\nTweet: {}'.format(tweet_data[doc_ix_with_word[0]]))\n",
    "\n",
    "# Document - term matrix relevant entry:\n",
    "document_row = doc_ix_with_word[0]\n",
    "vocab_col = vocab_interesting_ix \n",
    "mat_entry = doc_term_mat.tocsr()[document_row, vocab_col]\n",
    "\n",
    "print('\\nDocument-Term Matrix entry: {}'.format(mat_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_ix_with_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_data[doc_ix_with_word[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Trimming the Doc-term matrix\n",
    "\n",
    "We saw above that we are including terms like 'aaaaa' and 'aaaa', which probably occur very few times. These terms generally occur with unstructured text fields because we allow users to input whatever they feel like and that includes typos.  But be aware that they can also be artifacts of our cleaning process (unintentionally and intentionally).\n",
    "\n",
    "\n",
    "Since our document-term matrix is a matrix of counts of words (columns) in each document (rows), we want to remove words that don't occur very frequently across our corpus.\n",
    "\n",
    "The count of how frequent a word is in all of our corpus is just the sum of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  1 ...  6  1 12]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = doc_term_mat.sum(axis=0)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words w/counts above 15 : 6228\n"
     ]
    }
   ],
   "source": [
    "# Look at how many words are above a specific cutoff\n",
    "cutoff = 15\n",
    "word_count_list = word_counts.tolist()[0]\n",
    "# Find which column indices are above cutoff\n",
    "col_cutoff_ix = [ix for ix, count in enumerate(word_count_list) if count > cutoff]\n",
    "\n",
    "print('Number of words w/counts above {} : {}'.format(cutoff, len(col_cutoff_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix before trimming: (160000, 151670)\n",
      "Shape of document-term matrix after trimming: (160000, 6228)\n"
     ]
    }
   ],
   "source": [
    "# Get the trimmed vocabulary\n",
    "vocab_trimmed = np.array([vocab[x] for x in col_cutoff_ix])\n",
    "# Re-do the vocab-sorter\n",
    "vocab_sorter_trimmed = np.argsort(vocab_trimmed)\n",
    "\n",
    "print('Shape of document-term matrix before trimming: {}'.format(doc_term_mat.shape))\n",
    "\n",
    "# Trim the document-term matrix\n",
    "doc_term_mat_trimmed = doc_term_mat.tocsc()[:,col_cutoff_ix]\n",
    "\n",
    "print('Shape of document-term matrix after trimming: {}'.format(doc_term_mat_trimmed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'aa', 'aaa', 'aaaah', 'aaah', 'aah', 'aaron', 'ab',\n",
       "       'abandoned', 'abby'], dtype='<U37')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first 10 words alphabetically\n",
    "vocab_trimmed[vocab_sorter_trimmed[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Frequency\n",
    "How do we know what cutoff we should use?\n",
    "\n",
    "Let's look at a bar graph of frequency of words before and after we trimmed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count of words for each appearance\n",
    "hist_breaks = np.arange(0, 500, 10)\n",
    "plt.hist(word_count_list, bins = hist_breaks)\n",
    "plt.title('Histogram of word counts < 500')\n",
    "plt.show()\n",
    "\n",
    "# Too many words appear few times. Check out trimmed.\n",
    "trimmed_word_counts = doc_term_mat_trimmed.sum(axis=0)\n",
    "trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "hist_breaks = np.arange(0, 500, 5)\n",
    "plt.hist(trimmed_word_list, bins = hist_breaks)\n",
    "plt.title('Histogram of trimmed word counts < 500')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 15 might be a good cutoff because we get a visible drop off and for our next model, ~6000 features may not be too much.  These type of parameters, (cutoff), are **hyperparameters** for the model and will need tuning that is dependent on the model. You may consider trying different preprocessing steps and cutoffs and compare performances.\n",
    "\n",
    "The best way to find the cutoff is to test various model preprocessing through a train-test-validation split.  Train multiple versions and compare them on the test set.  Then report the best resulting model's performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "Now we have a matrix of data that we can consider as observations with our target to predict, 'sentiment'.  We could use each column (each column is an observance if a specific word occurs in a tweet) as a feature for predicting sentiment in a predictive model, e.g.- a logistic regression.\n",
    "\n",
    "This is done frequently, but we have a problem that word occurrences do not have a linear effect on much of anything. For example, the more frequently a word occurs, the less informative it can be (e.g., the word 'a' or 'the'), but words that rarely occur (e.g., 'aaaaa') are probably not very informative either. If we are interested in finding important words, we must look for words in between 'aaaaa' and 'a' in usage.\n",
    "\n",
    "The next idea we will explore is called 'TF-IDF', or 'Term Frequency-Inverse Document Frequency'.  Here we will be looking for rare words overall, but common inside individual documents.  It is these words that hold a lot of meaning to the document they reside in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Term Frequency\n",
    "\n",
    "Now that we have computed a document-term matrix, how can we understand it? Recall that the simple **Bag of Words model** is just based on **Term Frequency (TF)**. In this case, the weighting of a document for a given term is just the frequency of that term in the document. \n",
    "\n",
    "In other cases we will used the **Inverse Document Frequency (IDF)** weighting. IDF weighting accounts for cases where only a few documents contain certain terms. The formula for the IDF weighting can be written as:\n",
    "\n",
    "$$IDF = log(\\frac{Number\\ Documents}{Number\\ Documents\\ with\\ Word})$$\n",
    "\n",
    "The IDF can exhibit a problem however. When there are a few documents with very frequent terms, the weighting is skewed toward those documents.  To solve this problem, we reweight IDF by the overall frequency of the word to create a **term frequency-inverse document frequency (TF-IDF)** matrix. The formula for computing TFIDF is: \n",
    "\n",
    "$$TF - IDF = frequency(word) \\cdot log(\\frac{Number\\ Documents}{Number\\ Documents\\ with\\ Word})\\ $$\n",
    "\n",
    "The code in the cell below computes both simple TF and the cumulative of the term frequencies, starting from the most frequent terms to the least.\n",
    "\n",
    "Scikit-Learn has a built in TF-IDF transformation function that we will use to calculate this and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the TFIDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, max_features=6228, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer over the dataset\n",
    "clean_texts = tweet_df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<160000x6228 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 854733 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit learn prefers the 'csr' format instead (Compressed Sparse Row format)\n",
    "tf_idf_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Sentiment Analysis - Quiz Point 2\n",
    "\n",
    "Now that we have a prepared TDM of the 160,000 tweets, let's build and evaluate models to classify the sentiment of these tweets. An outline of our process is as follows:\n",
    "\n",
    "- Use TDM or TF-IDF weighted TDM as features for training the model.\n",
    "- Use marked cases for training and evaluation of model.\n",
    "- Select a method for sparse matrix requires regularization from the following:\n",
    "  - Feature selection, is impractical since there are over one million features.\n",
    "  - SVD/PCA could be used to reduce dimensionality of the problem.\n",
    "  - In this case we will use the ridge and lasso methods offered in the  elasticnet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will use 120,000 tweets to predict the 0,1 sentiment. The remaining 40,000 cases will be used to evaluate the model.\n",
    "\n",
    "Let's split the tf-idf sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into train-test. Please wait!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Generate 40,000 random row indices\n",
    "print('Splitting into train-test. Please wait!')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets,\n",
    "                                                    y_targets,\n",
    "                                                    test_size=40000,\n",
    "                                                    random_state=42)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a logistic classifier on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a standard Logistic Model training!\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ly266e\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "print('Starting a standard Logistic Model training!')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at model object\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7801666666666667\n",
      "Test accuracy: 0.7563\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coded in the cell below computes the precision, recall and Fscore of the model for positive and negative tweets. Execute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14597  5459]\n",
      " [ 4289 15655]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.77290056 0.74145117]\n",
      "Recall   : [0.72781213 0.78494785]\n",
      "F1       : [0.74967901 0.76257977]\n",
      "Support  : [20056 19944]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Remember:\n",
    "# Precision is the proportion of correct predictions among all predicted\n",
    "# Recall (sensitivity) is the proportion of correct predictions among all true actual examples\n",
    "# F1 is the harmonic average of precision and recall\n",
    "# Support is count of actual cases of specific class\n",
    "# Here, each of the following is a pair of numbers, the first is for class 1 ('1') and second for class 0 ('0')\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "\n",
    "# Get the parts of the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "# Print results\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "Let us try to improve the results with some regularization.  Here we will use elastic net type regulariation on our logistic regression to see if that helps.  Regularization can also help with the slight over training we have done.\n",
    "\n",
    "You can see that we have slightly over trained because our accuracy on our training set is greater than our test set.\n",
    "\n",
    "The way that Scikit-Learn solves for logistic regression is with a technique called stochastic gradient descent (SGD). We will need to import this and give the function some objective constraints to introduce elasticnet regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training regularized logistic regression\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Starting training regularized logistic regression')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "lr_reg = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)\n",
    "lr_reg.fit(X_train, y_train)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', penalty='elasticnet')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at model object\n",
    "lr_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr_reg.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr_reg.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7528583333333333\n",
      "Test accuracy: 0.7462\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our train and test accuracies are much closer together!  Although we lost ~1 percentage point in the accuracy, we might consider accepting this because it is less over fit to our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14170  5886]\n",
      " [ 4266 15678]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.7686049  0.72704508]\n",
      "Recall   : [0.70652174 0.78610108]\n",
      "F1       : [0.73625688 0.75542064]\n",
      "Support  : [20056 19944]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Remember:\n",
    "# Precision is the proportion of correct predictions among all predicted\n",
    "# Recall (sensitivity) is the proportion of correct predictions among all true actual examples\n",
    "# F1 is the harmonic average of precision and recall\n",
    "# Support is count of actual cases of specific class\n",
    "# Here, each of the following is a pair of numbers, the first is for class 1 ('1') and second for class 0 ('0')\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "\n",
    "# Get the parts of the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "# Print results\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Document Classification, Topic Models, and Information Retrieval\n",
    "\n",
    "A common objective of text analysis is to classify and group documents. These methods have application in information retrieval and search. Understandably, there are a great many such methods which have been developed over the years. We will only discuss a few examples in this tutorial.    \n",
    "\n",
    "- **Classification** is a widely used supervised learning method for document analysis. For example, documents can be classified as SPAM  or not SPAM or as positive or negative sentiment. \n",
    "- **Latent Semantic Analysis (LSA)** and **Doc-to-Vec** analysis are unsupervised learning methods used to determine which documents are closely related. These methods use similarity measures to rank documents as being related. These powerful methods are beyond the scope of this tutorial. \n",
    "- **Topic models** are an method wherein the a set of documents is categorized by one or more **topics**. analysis are unsupervised learning methods used to determine which documents are closely related. These methods use similarity measures to rank documents as being related.  \n",
    "  - **Latent Dirichlet Allocation (LDA)** allocates the probability that a document contains a topic.\n",
    "  - **Latent Semantic Analysis (LSA)** and **Doc-to-Vec**  These powerful methods are beyond the scope of this tutorial. \n",
    "- A variety of **distance metrics** have been developed to determine the distance between words, sentences and documents. These methods are related to coding theory widely used in telecommunications engineering.\n",
    "- **Clustering methods** are unsupervised learning models which seek to group similar documents into clusters. A variety of distance metrics can be used to define the structure of text clusters. \n",
    "  - K-means \n",
    "  - Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models\n",
    "\n",
    "It is often useful to allocate documents to one or more topics. This process can be useful in, say, information retrieval and search. Models to perform this allocation to topics are known as **topic models**. \n",
    "\n",
    "A powerful topic model is know as **Latent Dirichlet Allocation** or **LDA**. LDA is an unsupervised Bayesian learning model.  We can summarize the LDA model as follows:\n",
    "\n",
    "- The LDA model uses a fixed number of (sub) topics, k.\n",
    "- The model computes the posterior probability of a document containing a topic.\n",
    "- The model uses know word frequencies for documents in corpus, e.g. the tdm.\n",
    "- All other variables are estimated or **latent**, including the topics of each document. \n",
    "\n",
    "How does Latent Dirichlet allocation work? It's a Bayesian model, so we need to define a likelihood and choose a prior. \n",
    "\n",
    "Our posterior distribution is categorical, since we have many topics. The Dirichlet distribution is the conjugate of the multinomial and categorical distributions.\n",
    "\n",
    "All we actually know: $w_{ij}$ is the frequency of a specific word $j$ in document $i$.\n",
    "\n",
    "What we want to know (latent): $\\theta_i$ is the topic distribution of document $i$.\n",
    "\n",
    "We also need to estimate (latent):\n",
    "- $\\phi_k$ is the word distribution for topic $k$\n",
    "- $z_{ij}$ is the topic of the jth word in document $i$\n",
    "\n",
    "The Bayesian model and its priors:\n",
    "\n",
    "- Multinomial model\n",
    "$$z_{ij} \\sim Multinomial(\\theta_i)\\\\\n",
    "w_{ij} \\sim Multinomial(\\phi_k)$$\n",
    "\n",
    "- With Dirichlet priors with parameters $\\alpha$ and $\\beta$:\n",
    "$$\\theta_i \\sim Dirichelet(\\alpha)\\\\\n",
    "\\phi_k \\sim Dirichelet(\\beta)$$\n",
    "\n",
    "Since we don't know the allocation of topics in advance we generally use uniform priors across topics.\n",
    "\n",
    "The Likelihood is taken from the term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After trying multiple times I could not install the LDS package, therefore the following section was not explored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Example\n",
    "Let's try an example. In this example we apply LDA to a corpus of 20 business news articles from Reuters news wire concerning the oil industry. We will apply an LDA model with 5 topics ($k = 5$). \n",
    "\n",
    "As a first step we will load the corpus of these documents. Execute the code in the cell below to load the corpus and examine the contents of the first document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load the data set as a vector corpus of 20 documents\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# n_samples = 3000\n",
    "# n_features = 500\n",
    "# n_components = 10\n",
    "# n_top_words = 5\n",
    "\n",
    "# # Function below taken from Scikit Learn documentation\n",
    "# def print_top_words(model, feature_names, n_top_words):\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         message = \"Topic #{}: \".format(topic_idx)\n",
    "#         message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "#         print(message)\n",
    "#     print()\n",
    "\n",
    "\n",
    "# # Load the news dataset\n",
    "# print(\"Loading data set. Please wait.\")\n",
    "# news_data = fetch_20newsgroups(shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "# small_data = news_data.data[:n_samples]\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need the counts (frequencies of the words as well)\n",
    "\n",
    "# count_vectorizer = CountVectorizer(max_df=0.9, min_df=5, max_features=n_features, stop_words='english')\n",
    "# tf = count_vectorizer.fit_transform(small_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train LDA model\n",
    "# print('Starting LDA model training.')\n",
    "# lda = LatentDirichletAllocation(n_components, max_iter=5, learning_method='online', learning_offset=50.)\n",
    "# lda.fit(tf)\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Topics in LDA model:\")\n",
    "# tf_feature_names = count_vectorizer.get_feature_names()\n",
    "# print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 4\n",
    "True of False: \"The LDA topic model will come with the same key-words for the 10 topics above everytime.\"\n",
    "\n",
    "False. Well it is a difficult question. Provided it is the same data set and you run it under the same assumptions then yes, but then if you start adding more data or include more files then our prior distribution might change as a consequence the worlds for the topics we want to identify will change this as Latent Dirichlet allocation is a Bayesian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook you have worked through the following:\n",
    "- Quantifying text\n",
    "- Text Cleaning and Preprocessing\n",
    "- Classification and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Quiz Point</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
