{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10 - Sentiment Analysis\n",
    "\n",
    "## Author: Rodolfo Lerma\n",
    "\n",
    "This assignment requires that you build a sentiment analysis classifier for a series of tweets.\n",
    "The data consists of a file \"twitter_data.csv\". The file contains 16,000 tweets with their respective score. The attributes are the sentences, and the score is either 4 (for positive) or 0 (for negative).\n",
    "\n",
    "Assignment Instructions\n",
    "1. Complete all questions below.\n",
    "2. Comment on the applicability of the model on future tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text\n",
      "0                4  @elephantbird Hey dear, Happy Friday to You  A...\n",
      "1                4  Ughhh layin downnnn    Waiting for zeina to co...\n",
      "2                0  @greeniebach I reckon he'll play, even if he's...\n",
      "3                0              @vaLewee I know!  Saw it on the news!\n",
      "4                0  very sad that http://www.fabchannel.com/ has c...\n"
     ]
    }
   ],
   "source": [
    "#Read files\n",
    "url = \"twitter_data.csv\"\n",
    "df = pd.read_csv(url, sep=\",\")\n",
    "df.columns = [\"sentiment_label\",\"tweet_text\"]\n",
    "    \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text\n",
      "0                1  @elephantbird Hey dear, Happy Friday to You  A...\n",
      "1                1  Ughhh layin downnnn    Waiting for zeina to co...\n",
      "2                0  @greeniebach I reckon he'll play, even if he's...\n",
      "3                0              @vaLewee I know!  Saw it on the news!\n",
      "4                0  very sad that http://www.fabchannel.com/ has c...\n"
     ]
    }
   ],
   "source": [
    "# But sentiment is either '4' or '0'. We'll change that to '1' or '0' to indicate positive or negative sentiment.\n",
    "df.sentiment_label=df.sentiment_label.replace(4,1)\n",
    "\n",
    "# Check the Data frame again\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.sentiment_label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Generate word cloud for positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Count of positives: 80000\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n Count of positives: {}'.format(np.sum(df['sentiment_label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Generate word cloud for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Count of negative: 80000\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n Count of negative: {}'.format(len(df['sentiment_label']) - np.sum(df['sentiment_label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Split data into 70% for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to do this\n",
    "import string\n",
    "import re\n",
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "#steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace']\n",
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace', 'stem_words']\n",
    "\n",
    "df['clean_tweet'] = df['tweet_text'].map(lambda s: preprocess(s, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a document storage matrix\n",
    "# clean_texts = df['clean_tweet']\n",
    "# docs = {}\n",
    "# labels = []\n",
    "# for ix, row in enumerate(clean_texts):\n",
    "#     # Store the sentiment\n",
    "#     labels = tweet_data[ix][0]\n",
    "#     docs[ix] = row.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We want to keep track of how many unique words there are:\n",
    "# num_nonzero = 0\n",
    "# vocab = set()\n",
    "\n",
    "# for word_list in docs.values():\n",
    "#     unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "#     vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "#     num_nonzero += len(unique_terms) # add count of unique terms in this tweet\n",
    "\n",
    "# doc_key_list = list(docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to convert everything to a numpy array:\n",
    "# doc_key_list = np.array(doc_key_list)\n",
    "# vocab = np.array(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We should keep track of how the vocab/term indices map to the matrix so that we can look them up later.\n",
    "# vocab_sorter = np.argsort(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize our sparse matrix:\n",
    "# num_docs = len(doc_key_list)\n",
    "# vocab_size = len(vocab)\n",
    "# # A COO matrix is just a tuple of data, row indices, and column indices. Everything else is assumed to be zero.\n",
    "# data = np.empty(num_nonzero, dtype=np.intc)     # all non-zero\n",
    "# rows = np.empty(num_nonzero, dtype=np.intc)     # row index\n",
    "# cols = np.empty(num_nonzero, dtype=np.intc)     # column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix = 0\n",
    "# # go through all documents with their terms\n",
    "# print('Computing full term-document matrix (sparse), please wait!')\n",
    "# for doc_key, terms in docs.items():\n",
    "#     # find indices to insert-into such that, if the corresponding elements were\n",
    "#     # inserted before the indices, the order would be preserved\n",
    "#     term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "#     # count the unique terms of the document and get their vocabulary indices\n",
    "#     uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "#     n_vals = len(uniq_indices)  # = number of unique terms\n",
    "#     ix_end = ix + n_vals # Add count to index.\n",
    "\n",
    "#     data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "#     cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "#     doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "#     rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "\n",
    "#     ix = ix_end  # resume with next document -> will add future data on the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the sparse matrix!\n",
    "# doc_term_mat = coo_matrix((data, (rows, cols)), shape=(num_docs, vocab_size), dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's check to make sure!\n",
    "# vocab_list = list(vocab)\n",
    "# word_of_interest = 'math'\n",
    "# vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "# print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# # Find which tweets contain word:\n",
    "# doc_ix_with_word = []\n",
    "# for ix, row in enumerate(tweet_data): # Note on this line later.\n",
    "#     if word_of_interest in row[1]:\n",
    "#         doc_ix_with_word.append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "      <td>elephantbird hey dear happy friday to you alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "      <td>ughhh layin downnnn waiting for zeina to cook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "      <td>greeniebach i reckon hell play even if he not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "      <td>valewee i know saw it on the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "      <td>very sad that httpwwwfabchannelcom ha closed d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text  \\\n",
       "0                1  @elephantbird Hey dear, Happy Friday to You  A...   \n",
       "1                1  Ughhh layin downnnn    Waiting for zeina to co...   \n",
       "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
       "3                0              @vaLewee I know!  Saw it on the news!   \n",
       "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  elephantbird hey dear happy friday to you alre...  \n",
       "1  ughhh layin downnnn waiting for zeina to cook ...  \n",
       "2  greeniebach i reckon hell play even if he not ...  \n",
       "3                  valewee i know saw it on the news  \n",
       "4  very sad that httpwwwfabchannelcom ha closed d...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the TFIDF vectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, max_features=6228, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer over the dataset\n",
    "clean_texts = df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into train-test. Please wait!\n"
     ]
    }
   ],
   "source": [
    "print('Splitting into train-test. Please wait!')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Build a classifier that classifies the sentiment of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a standard Logistic Model training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting a standard Logistic Model training!')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training regularized logistic regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', penalty='elasticnet')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting training regularized logistic regression')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "lr_reg = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)\n",
    "lr_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: What is the accuracy of your model when applied to testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7798482142857143\n",
      "Test accuracy: 0.7548125\n",
      "[[17505  6539]\n",
      " [ 5230 18726]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.76995821 0.74118346]\n",
      "Recall   : [0.72804026 0.78168309]\n",
      "F1       : [0.74841275 0.76089474]\n",
      "Support  : [24044 23956]\n"
     ]
    }
   ],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)\n",
    "\n",
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Remember:\n",
    "# Precision is the proportion of correct predictions among all predicted\n",
    "# Recall (sensitivity) is the proportion of correct predictions among all true actual examples\n",
    "# F1 is the harmonic average of precision and recall\n",
    "# Support is count of actual cases of specific class\n",
    "# Here, each of the following is a pair of numbers, the first is for class 1 ('1') and second for class 0 ('0')\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "\n",
    "# Get the parts of the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "# Print results\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7516517857142857\n",
      "Test accuracy: 0.7437708333333334\n",
      "[[16852  7192]\n",
      " [ 5107 18849]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.76743021 0.72382013]\n",
      "Recall   : [0.70088172 0.7868175 ]\n",
      "F1       : [0.73264787 0.75400524]\n",
      "Support  : [24044 23956]\n"
     ]
    }
   ],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr_reg.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr_reg.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)\n",
    "\n",
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Remember:\n",
    "# Precision is the proportion of correct predictions among all predicted\n",
    "# Recall (sensitivity) is the proportion of correct predictions among all true actual examples\n",
    "# F1 is the harmonic average of precision and recall\n",
    "# Support is count of actual cases of specific class\n",
    "# Here, each of the following is a pair of numbers, the first is for class 1 ('1') and second for class 0 ('0')\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "\n",
    "# Get the parts of the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "# Print results\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: What conclusions can you draw from the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Is it better to have a model per source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
